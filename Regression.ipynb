{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 506\n",
      "target 506\n",
      "feature_names 13\n",
      "DESCR 2379\n"
     ]
    }
   ],
   "source": [
    "for key, value in dataset.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
      "       6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
      "       1.7800e+01, 3.9690e+02, 9.1400e+00]), 21.6)\n",
      "(array([1.1747e-01, 1.2500e+01, 7.8700e+00, 0.0000e+00, 5.2400e-01,\n",
      "       6.0090e+00, 8.2900e+01, 6.2267e+00, 5.0000e+00, 3.1100e+02,\n",
      "       1.5200e+01, 3.9690e+02, 1.3270e+01]), 18.9)\n",
      "(array([1.0084e-01, 0.0000e+00, 1.0010e+01, 0.0000e+00, 5.4700e-01,\n",
      "       6.7150e+00, 8.1600e+01, 2.6775e+00, 6.0000e+00, 4.3200e+02,\n",
      "       1.7800e+01, 3.9559e+02, 1.0160e+01]), 22.8)\n"
     ]
    }
   ],
   "source": [
    "for index, datum in enumerate(zip(dataset['data'], dataset['target'])):\n",
    "    if index in [1, 11, 111]:\n",
    "        print(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.asarray(dataset['data'])\n",
    "y = np.asarray(dataset['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso (a=0.1): 0.71409\n",
      "Lasso (a=0.01): 0.73324\n"
     ]
    }
   ],
   "source": [
    "#L1\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#Models\n",
    "Lasso_01 = Lasso(alpha=0.1)\n",
    "Lasso_001 = Lasso(alpha=0.01)\n",
    "\n",
    "#Fitting\n",
    "Lasso_01.fit(X_train, y_train)\n",
    "Lasso_001.fit(X_train, y_train)\n",
    "\n",
    "#Score\n",
    "print(\"Lasso (a=0.1): {:.5f}\".format(Lasso_01.score(X_test, y_test)))\n",
    "print(\"Lasso (a=0.01): {:.5f}\".format(Lasso_001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients list: \n",
      "[-0.02  0.05 -0.    0.   -0.    1.    0.02 -0.6   0.23 -0.02 -0.62  0.01\n",
      " -0.74]\n",
      "\n",
      "Indexes of important features: \n",
      "[ 5  7 10 12]\n"
     ]
    }
   ],
   "source": [
    "#Let's find the important features!\n",
    "Lasso_feature_selection = Lasso(alpha=1)\n",
    "Lasso_feature_selection.fit(X_train, y_train)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "abs_feat = np.abs(Lasso_feature_selection.coef_)\n",
    "perc_95 = np.percentile(Lasso_feature_selection.coef_, 95)\n",
    "\n",
    "print(\"Coefficients list: \\n{}\\n\".format(Lasso_feature_selection.coef_))\n",
    "print(\"Indexes of important features: \\n{}\".format(np.argwhere(abs_feat > perc_95).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso (using 4 important features): 0.68186\n",
      "Lasso (using all features): 0.71409\n"
     ]
    }
   ],
   "source": [
    "# 5f: average number of rooms per dwelling\n",
    "# 7f: weighted distances to five Boston employment centres\n",
    "# 10f: pupil-teacher ratio by town\n",
    "# 12f: % lower status of the population\n",
    "New_Lasso = Lasso(alpha=0.1)\n",
    "New_Lasso.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "print(\"Lasso (using 4 important features): {:.5f}\".format(New_Lasso.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"Lasso (using all features): {:.5f}\".format(Lasso_01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge (using 4 important features): 0.68311\n",
      "Ridge (using all features): 0.73414\n"
     ]
    }
   ],
   "source": [
    "#L2\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#Models\n",
    "Ridge_all = Ridge(alpha=0.1)\n",
    "Ridge_part = Ridge(alpha=0.1)\n",
    "\n",
    "#Fitting\n",
    "Ridge_all.fit(X_train, y_train)\n",
    "Ridge_part.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "\n",
    "#Score\n",
    "print(\"Ridge (using 4 important features): {:.5f}\".format(Ridge_part.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"Ridge (using all features): {:.5f}\".format(Ridge_all.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet (using 4 important features): 0.68071\n",
      "ElasticNet (using all features): 0.71548\n"
     ]
    }
   ],
   "source": [
    "#L1 & L2 linear combination\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#Models\n",
    "EN_all = ElasticNet(alpha=0.1)\n",
    "EN_part = ElasticNet(alpha=0.1)\n",
    "\n",
    "#Fitting\n",
    "EN_all.fit(X_train, y_train)\n",
    "EN_part.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "\n",
    "#Score\n",
    "print(\"ElasticNet (using 4 important features): {:.5f}\".format(EN_part.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"ElasticNet (using all features): {:.5f}\".format(EN_all.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Linear (4 f.): 0.67233\n",
      "SVM Linear (all f.): 0.69492\n",
      "\n",
      "SVM RBF (4 f.): 0.67926\n",
      "SVM RBF (all f.): 0.69492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Models\n",
    "SVM_rbf_all = make_pipeline(StandardScaler(), SVR(kernel='rbf'))\n",
    "SVM_rbf_part = make_pipeline(StandardScaler(), SVR(kernel='rbf'))\n",
    "\n",
    "SVM_linear_all = make_pipeline(StandardScaler(), SVR(kernel='linear'))\n",
    "SVM_linear_part = make_pipeline(StandardScaler(), SVR(kernel='linear'))\n",
    "\n",
    "#Fitting\n",
    "SVM_rbf_all.fit(X_train, y_train)\n",
    "SVM_rbf_part.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "\n",
    "SVM_linear_all.fit(X_train, y_train)\n",
    "SVM_linear_part.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "\n",
    "#Score\n",
    "print(\"SVM Linear (4 f.): {:.5f}\".format(SVM_linear_part.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"SVM Linear (all f.): {:.5f}\\n\".format(SVM_linear_all.score(X_test, y_test)))\n",
    "\n",
    "print(\"SVM RBF (4 f.): {:.5f}\".format(SVM_rbf_part.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"SVM RBF (all f.): {:.5f}\".format(SVM_linear_all.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted Decision Tree (4 f.): 0.85291\n",
      "Boosted Decision Tree (all f.): 0.82870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#Models\n",
    "Ada_all = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=50)\n",
    "Ada_part = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=50)\n",
    "\n",
    "#Fitting\n",
    "Ada_all.fit(X_train, y_train)\n",
    "Ada_part.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "\n",
    "#Score\n",
    "print(\"Boosted Decision Tree (4 f.): {:.5f}\".format(Ada_part.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"Boosted Decision Tree (all f.): {:.5f}\\n\".format(Ada_all.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (4 f.): 0.81941\n",
      "Random Forest (all f.): 0.78890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Models\n",
    "RF_all = RandomForestRegressor(bootstrap=True, n_estimators=50)\n",
    "RF_part = RandomForestRegressor(bootstrap=True, n_estimators=50)\n",
    "\n",
    "#Fitting\n",
    "RF_all.fit(X_train, y_train)\n",
    "RF_part.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "\n",
    "#Score\n",
    "print(\"Random Forest (4 f.): {:.5f}\".format(RF_part.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"Random Forest (all f.): {:.5f}\\n\".format(RF_all.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting (4 f.): 0.86142\n",
      "Gradient Boosting (all f.): 0.87053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Models\n",
    "GB_all = GradientBoostingRegressor(n_estimators=50)\n",
    "GB_part = GradientBoostingRegressor(n_estimators=50)\n",
    "\n",
    "#Fitting\n",
    "GB_all.fit(X_train, y_train)\n",
    "GB_part.fit(X_train[:, (5,7,10,12)], y_train)\n",
    "\n",
    "#Score\n",
    "print(\"Gradient Boosting (4 f.): {:.5f}\".format(GB_part.score(X_test[:, (5,7,10,12)], y_test)))\n",
    "print(\"Gradient Boosting (all f.): {:.5f}\\n\".format(GB_all.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
